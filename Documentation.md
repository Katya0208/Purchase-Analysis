
# Документация по проекту "Анализ покупок в магазине"

## 1. Обзор проекта

**Цель проекта:**  
Создать систему, которая объединяет данные о клиентах, товарах, покупках и продавцах из разных источников для оперативной аналитики и визуализации. Система включает:

- **PostgreSQL** – хранение данных о клиентах.
    
- **S3** с использованием Iceberg/Hudi/Delta Lake – хранение данных о товарах.
    
- **Kafka** – обработка событий о покупках в режиме реального времени.
    
- **ETL и Stage слой** – единое промежуточное хранилище данных.
    
- **ClickHouse** – аналитическая база данных для формирования отчётов.
    
- **Spark** – batch обработка для обновления данных.
    
- **API** – пополнение и обновление данных о продавцах, товарах и покупках.
    

---

## 2. Общая словарная спецификация

**Ключевые сущности:**

- **Client (Клиент)**
    
- **Product (Товар)**
    
- **Purchase (Покупка)**
    
- **Seller (Продавец)**
    

**Общие поля для всех сущностей:**

- **Идентификаторы:**
    
    - `client_id`, `product_id`, `purchase_id`, `seller_id` – тип: UUID (или целочисленное значение, если принято решение использовать его).
        
- **Временные метки:**
    
    - Поля типа `timestamp` или `created_at` в формате ISO 8601 (например, `2025-04-06T12:00:00Z`).
        

---

## 3. Схема данных для PostgreSQL (данные о клиентах)

**Таблица: clients**

```sql
CREATE TABLE clients (
    client_id UUID PRIMARY KEY,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    email VARCHAR(150),
    address VARCHAR(250),
    registration_date TIMESTAMPTZ
);
```

- **Описание:**  
    Хранит информацию о клиентах: идентификатор, имя, фамилия, email, адрес и дату регистрации.
    
- **Форматы данных:**  
    Используются стандартные SQL типы, идентификатор – UUID, дата/время – TIMESTAMPTZ.
    

---

## 4. Форматы сообщений для Kafka (данные о покупках)

**Формат сообщения – JSON**

```json
{
  "purchase_id": "123e4567-e89b-12d3-a456-426614174000",
  "client_id": "987e6543-e21b-45d3-b123-526714174000",
  "product_id": "543e2198-e89b-12d3-a456-426614174000",
  "quantity": 2,
  "price": 49.99,
  "timestamp": "2025-04-06T12:00:00Z"
}
```

- **Описание:**  
    Каждое сообщение отражает совершённую покупку.
    
- **Валидация:**  
    Рекомендуется использовать JSON Schema или Avro для проверки структуры и типов данных.
    
- **Ключевые моменты:**
    
    - Поля `purchase_id`, `client_id`, `product_id` должны соответствовать форматам в других системах.
        
    - Временная метка `timestamp` в ISO 8601.
        

---

## 5. Форматы хранения данных на S3 (данные о товарах)

**Хранение данных – S3 с Iceberg/Hudi/Delta Lake**  
Внутри этих систем данные сохраняются в формате Parquet или ORC, но используются дополнительные метаданные для управления версионностью.

**Пример Avro-схемы для товаров:**

```json
{
  "type": "record",
  "name": "Product",
  "fields": [
    {"name": "product_id", "type": "string"},
    {"name": "name", "type": "string"},
    {"name": "category", "type": "string"},
    {"name": "price", "type": "double"},
    {"name": "available_stock", "type": "int"},
    {"name": "updated_at", "type": {"type": "long", "logicalType": "timestamp-millis"}}
  ]
}
```

- **Описание:**  
    Схема определяет, как хранятся данные о товарах: идентификатор, название, категория, цена, остаток на складе и время обновления.
    
- **Использование:**  
    Для обеспечения согласованности и поддержки ACID-транзакций в больших данных.
    

---

## 6. Stage слой: унифицированное хранилище данных

**Цель:**  
Собрать и привести данные из различных источников (PostgreSQL, Kafka, S3) к единому формату для дальнейшей обработки.

**Унифицированная структура записи для аналитики:**

```json
{
  "client": {
    "client_id": "987e6543-e21b-45d3-b123-526714174000",
    "first_name": "Иван",
    "last_name": "Иванов",
    "email": "ivan@example.com"
  },
  "purchase": {
    "purchase_id": "123e4567-e89b-12d3-a456-426614174000",
    "product_id": "543e2198-e89b-12d3-a456-426614174000",
    "quantity": 2,
    "price": 49.99,
    "timestamp": "2025-04-06T12:00:00Z"
  },
  "product": {
    "product_id": "543e2198-e89b-12d3-a456-426614174000",
    "name": "Смартфон",
    "category": "Электроника",
    "price": 499.99
  }
}
```

- **Описание:**  
    Данные из всех источников объединяются для создания полной картины по каждой транзакции.
    

---

## 7. Batch обработка данных (Spark)

**Цель:**  
Ежедневное обновление данных о клиентах и товарах.

**Основные шаги:**

- Чтение данных из Stage слоя.
    
- Преобразование и агрегация данных.
    
- Запись обновлённых данных в целевые хранилища.
    

**Требования:**

- Согласовать формат данных, поступающих в Stage слой (см. раздел 6).
    
- Определить расписание обновления (например, раз в сутки).
    

---

## 8. Стриминговая обработка (Kafka → ClickHouse)

**Цель:**  
Обработка событий о покупках в реальном времени и запись данных в ClickHouse для аналитики.

**Основные шаги:**

- Чтение сообщений из Kafka.
    
- Преобразование данных (возможно, агрегация, фильтрация).
    
- Запись обработанных данных в ClickHouse.
    

**Требования:**

- Формат сообщений в Kafka должен соответствовать JSON-схеме (см. раздел 4).
    
- ClickHouse должен принимать данные с теми же ключевыми полями (например, `purchase_id`, `timestamp`).
    

---

## 9. API для пополнения и обновления данных

**Общие требования:**  
API должен поддерживать добавление и обновление данных для сущностей: продавцы, товары, покупки.

**Пример спецификации для добавления покупки:**

- **Endpoint:** `POST /api/purchases`
    
- **Тело запроса (JSON):**
    
    ```json
    {
      "purchase_id": "123e4567-e89b-12d3-a456-426614174000",
      "client_id": "987e6543-e21b-45d3-b123-526714174000",
      "product_id": "543e2198-e89b-12d3-a456-426614174000",
      "quantity": 2,
      "price": 49.99,
      "timestamp": "2025-04-06T12:00:00Z"
    }
    ```
    
- **Валидация:**
    
    - Проверка обязательных полей.
        
    - Соответствие типов (UUID для идентификаторов, число для количества и цены, корректный формат даты).
        
- **Документация:**
    
    - Использование OpenAPI для описания всех endpoints, схем запросов и ответов.
        
    - Пример OpenAPI спецификации:
        
        ```yaml
        openapi: 3.0.0
        info:
          title: API для пополнения данных
          version: 1.0.0
        paths:
          /api/purchases:
            post:
              summary: Добавить покупку
              requestBody:
                required: true
                content:
                  application/json:
                    schema:
                      type: object
                      properties:
                        purchase_id:
                          type: string
                          format: uuid
                        client_id:
                          type: string
                          format: uuid
                        product_id:
                          type: string
                          format: uuid
                        quantity:
                          type: integer
                        price:
                          type: number
                        timestamp:
                          type: string
                          format: date-time
                      required:
                        - purchase_id
                        - client_id
                        - product_id
                        - quantity
                        - price
                        - timestamp
              responses:
                '200':
                  description: Покупка успешно добавлена
                '400':
                  description: Неверный формат данных
        ```
        

---

## 10. Согласование и управление версионностью

**Рекомендации:**

- **Единый набор полей:**  
    Все компоненты (PostgreSQL, Kafka, S3, API) используют одни и те же названия полей и типы данных (см. раздел 2).
    
- **Формат временных меток:**  
    Использование ISO 8601 для всех временных полей.
    
- **Схемы и валидация:**  
    Применение JSON Schema, Avro или Protobuf для проверки формата данных на этапе передачи.
    
- **Документация и репозиторий:**  
    Все схемы и спецификации должны храниться в общем репозитории (например, Confluence, Git), чтобы каждый участник проекта имел к ним доступ.
    
- **Версионность:**  
    При внесении изменений в схемы данных обеспечить поддержку версионности (например, с помощью schema registry для Kafka).
    

---

## Заключение

Данная документация представляет собой единый стандарт для согласования форматов и структур данных между всеми компонентами проекта.

- **ETL и Stage слой:** данные собираются и унифицируются в единую структуру.
    
- **Стриминг и Batch обработка:** данные обрабатываются в реальном времени и пакетами с соблюдением единой схемы.
    
- **API:** обеспечивает единообразное пополнение и обновление данных, с соблюдением согласованных форматов.
    

Эта документация поможет обеспечить совместимость между компонентами, снизить вероятность ошибок интеграции и повысить качество итогового продукта. Если возникнут дополнительные вопросы или потребуется уточнение отдельных деталей, дайте знать.